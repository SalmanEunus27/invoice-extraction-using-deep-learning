{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_3VezKeGFGZk",
        "outputId": "7bba8fb3-4e5f-4fe3-d320-57b7db3823c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytesseract\n",
            "  Downloading pytesseract-0.3.8.tar.gz (14 kB)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from pytesseract) (7.1.2)\n",
            "Building wheels for collected packages: pytesseract\n",
            "  Building wheel for pytesseract (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytesseract: filename=pytesseract-0.3.8-py2.py3-none-any.whl size=14070 sha256=e39c4eef74b04e5e4c7da20d307bb8a2514a0db88aed65d8526f8aa6453ec4dd\n",
            "  Stored in directory: /root/.cache/pip/wheels/a4/89/b9/3f11250225d0f90e5454fcc30fd1b7208db226850715aa9ace\n",
            "Successfully built pytesseract\n",
            "Installing collected packages: pytesseract\n",
            "Successfully installed pytesseract-0.3.8\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  tesseract-ocr-eng tesseract-ocr-osd\n",
            "The following NEW packages will be installed:\n",
            "  tesseract-ocr tesseract-ocr-eng tesseract-ocr-osd\n",
            "0 upgraded, 3 newly installed, 0 to remove and 37 not upgraded.\n",
            "Need to get 4,795 kB of archives.\n",
            "After this operation, 15.8 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 tesseract-ocr-eng all 4.00~git24-0e00fe6-1.2 [1,588 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 tesseract-ocr-osd all 4.00~git24-0e00fe6-1.2 [2,989 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/universe amd64 tesseract-ocr amd64 4.00~git2288-10f4998a-2 [218 kB]\n",
            "Fetched 4,795 kB in 1s (5,453 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 3.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package tesseract-ocr-eng.\n",
            "(Reading database ... 155229 files and directories currently installed.)\n",
            "Preparing to unpack .../tesseract-ocr-eng_4.00~git24-0e00fe6-1.2_all.deb ...\n",
            "Unpacking tesseract-ocr-eng (4.00~git24-0e00fe6-1.2) ...\n",
            "Selecting previously unselected package tesseract-ocr-osd.\n",
            "Preparing to unpack .../tesseract-ocr-osd_4.00~git24-0e00fe6-1.2_all.deb ...\n",
            "Unpacking tesseract-ocr-osd (4.00~git24-0e00fe6-1.2) ...\n",
            "Selecting previously unselected package tesseract-ocr.\n",
            "Preparing to unpack .../tesseract-ocr_4.00~git2288-10f4998a-2_amd64.deb ...\n",
            "Unpacking tesseract-ocr (4.00~git2288-10f4998a-2) ...\n",
            "Setting up tesseract-ocr-osd (4.00~git24-0e00fe6-1.2) ...\n",
            "Setting up tesseract-ocr-eng (4.00~git24-0e00fe6-1.2) ...\n",
            "Setting up tesseract-ocr (4.00~git2288-10f4998a-2) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n"
          ]
        }
      ],
      "source": [
        "!pip install pytesseract\n",
        "!sudo apt install tesseract-ocr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GlvVVLiKnqSJ",
        "outputId": "d46251b5-80fc-4476-a47b-e3fdadc199d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f067dfc8eb0>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from xml.etree import ElementTree\n",
        "import numpy as np\n",
        "import cv2\n",
        "from matplotlib.pyplot import imshow\n",
        "from matplotlib import cm\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.utils.data\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import transforms\n",
        "\n",
        "\n",
        "# import libraries\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "try:\n",
        " from PIL import Image\n",
        "except ImportError:\n",
        " import Image\n",
        "import cv2\n",
        "import pytesseract\n",
        "import numpy as np\n",
        "from pytesseract import Output\n",
        "\n",
        "pytesseract.pytesseract.tesseract_cmd = (\n",
        "    r'/usr/bin/tesseract'\n",
        ")\n",
        "\n",
        "from nltk.tokenize import  word_tokenize \n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "\n",
        "import gensim\n",
        "from gensim import corpora\n",
        "\n",
        "import argparse\n",
        "import logging\n",
        "import os\n",
        "import sys\n",
        "\n",
        "\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "\n",
        "import time\n",
        "\n",
        "random.seed(1)\n",
        "np.random.seed(1)\n",
        "torch.manual_seed(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yi_I8V29n1m3"
      },
      "outputs": [],
      "source": [
        "IMAGE_SIZE = 300\n",
        "# path to directory with receipts and xmls directories\n",
        "# data_path\n",
        "#   |_receipts\n",
        "#   |_xmls (Output of LabelImg software)\n",
        "data_path = \"/content/sample_data/invoices\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P0J7IedzH_s2"
      },
      "outputs": [],
      "source": [
        "class ReceiptsDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, root, transforms=None):\n",
        "\n",
        "        self.root = root\n",
        "        self.transforms = transforms\n",
        "\n",
        "        self.imgs = sorted([path for path in os.listdir(os.path.join(root, \"receipts\")) if path.endswith(\".jpg\")])\n",
        "\n",
        "        #load bounding coordinates\n",
        "        self.xmls = sorted([path for path in os.listdir(os.path.join(root, \"xmls\")) if path.endswith(\".xml\")])\n",
        "\n",
        "        len_min = min(len(self.imgs), len(self.xmls))\n",
        "        self.imgs = self.imgs[:len_min]\n",
        "        self.xmls = self.xmls[:len_min]\n",
        "\n",
        "        self.dictionary = corpora.Dictionary([])\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        \"\"\"\n",
        "        This function is the core component of dataset structure, as we read the \n",
        "        image build its corresponding mask and grid.\n",
        " \n",
        "        \"\"\"\n",
        "\n",
        "        # load images ad masks\n",
        "        img_path = os.path.join(os.path.join(self.root, \"/content/sample_data/invoices/receipts\"), self.imgs[idx])\n",
        "        xml_path = os.path.join(os.path.join(self.root, \"/content/sample_data/invoices/xmls\"), self.xmls[idx])\n",
        "\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        w,h = img.size\n",
        "\n",
        "        # 0: other, 1: total: boxes\n",
        "        boxes = get_boxes(xml_path)\n",
        "        grid, self.dictionary = get_grid(img, self.dictionary)\n",
        "        mask = get_mask(boxes, w, h)\n",
        "\n",
        "        mask = Image.fromarray(np.uint8(cm.gist_earth(mask)*255)).convert(\"L\")\n",
        "\n",
        "        grid = Image.fromarray(np.uint8(cm.gist_earth(grid)*255)).convert(\"L\")\n",
        "\n",
        "        if self.transforms is not None:\n",
        "            img, grid, mask = self.transforms(img, grid, mask)\n",
        "\n",
        "        mask = np.where(np.array(mask)>0,1,0)\n",
        "\n",
        "\n",
        "        img = torch.as_tensor(np.array(img), dtype=torch.float32)\n",
        "        mask = torch.as_tensor(mask, dtype=torch.long).unsqueeze(0)\n",
        "        grid = torch.as_tensor(np.array(grid), dtype=torch.long)\n",
        "\n",
        "        return img, grid, mask\n",
        "        \n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XTzpe9BtIr7s"
      },
      "outputs": [],
      "source": [
        "class CUTIE(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(CUTIE, self).__init__()\n",
        "    self.OUT_CHANNELS = 32\n",
        "    self.EMBEDDING_SIZE = EMBEDDING_SIZE\n",
        "    self.NB_CLASS = NB_CLASS\n",
        "\n",
        "\n",
        "    #grid\n",
        "    self.embedding_layer = nn.Embedding(VOCAB_SIZE, EMBEDDING_SIZE)\n",
        "    self.avgPool = nn.AdaptiveAvgPool2d((64,64))\n",
        "    self.avgPoolFinal = nn.AdaptiveAvgPool2d((IMAGE_SIZE,IMAGE_SIZE))\n",
        "\n",
        "\n",
        "    self.conv1_1 = nn.Conv2d(self.EMBEDDING_SIZE+self.OUT_CHANNELS,self.EMBEDDING_SIZE+self.OUT_CHANNELS, 1, 1 )\n",
        "    self.conv1_1_layer_2 = nn.Conv2d(4*self.OUT_CHANNELS,4*self.OUT_CHANNELS, 1, 1 )\n",
        "    self.conv_layer_3 = nn.Conv2d(5*self.OUT_CHANNELS,16, 3, 1, 1 )\n",
        "    self.conv1_1_layer_4 = nn.Conv2d(16, self.NB_CLASS, 1, 1 )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    #image\n",
        "    self.conv_layer_1 = nn.Sequential(\n",
        "        nn.Conv2d(IN_CHANNELS,self.OUT_CHANNELS,3, stride=1),\n",
        "        nn.BatchNorm2d(self.OUT_CHANNELS),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "\n",
        "    self.conv_block = nn.Sequential(\n",
        "          nn.Conv2d(self.OUT_CHANNELS,self.OUT_CHANNELS,3, stride=1),\n",
        "          nn.BatchNorm2d(self.OUT_CHANNELS),\n",
        "          \n",
        "          nn.ReLU(),\n",
        "          \n",
        "          nn.Conv2d(self.OUT_CHANNELS,self.OUT_CHANNELS,3, stride=1),\n",
        "          nn.BatchNorm2d(self.OUT_CHANNELS),\n",
        "          nn.ReLU(),\n",
        "          \n",
        "          nn.Conv2d(self.OUT_CHANNELS,self.OUT_CHANNELS,3, stride=2),\n",
        "          nn.BatchNorm2d(self.OUT_CHANNELS),\n",
        "          nn.ReLU()\n",
        "        )\n",
        "    \n",
        "    self.atrous_block = nn.Sequential(\n",
        "        nn.Conv2d(self.OUT_CHANNELS+self.EMBEDDING_SIZE, self.OUT_CHANNELS, 3, stride = 1, padding = 0, dilation=2),\n",
        "        nn.ReLU(),\n",
        "        \n",
        "        nn.Conv2d(self.OUT_CHANNELS, self.OUT_CHANNELS, 3, stride = 1, padding = 0, dilation=2),\n",
        "        nn.ReLU(),\n",
        "        \n",
        "        nn.Conv2d(self.OUT_CHANNELS, self.OUT_CHANNELS, 3, stride = 1, padding = 0, dilation=2),\n",
        "        nn.ReLU(),\n",
        "\n",
        "        nn.Conv2d(self.OUT_CHANNELS, self.OUT_CHANNELS, 3, stride = 1, padding = 0, dilation=2),\n",
        "        nn.BatchNorm2d(self.OUT_CHANNELS),\n",
        "        nn.ReLU()\n",
        "  \n",
        "    )\n",
        "\n",
        "\n",
        "    #aspp\n",
        "    self.aspp_layer_1 = nn.Conv2d(self.OUT_CHANNELS, self.OUT_CHANNELS, 7, stride=1, dilation=4)\n",
        "    self.aspp_layer_2 = nn.Conv2d(self.OUT_CHANNELS, self.OUT_CHANNELS, 5, padding=1, dilation=8)\n",
        "    self.aspp_layer_3 = nn.Conv2d(self.OUT_CHANNELS, self.OUT_CHANNELS, 3, padding=1, dilation=16)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self, images, grids):\n",
        "\n",
        "    # grid\n",
        "    embed = self.embedding_layer(grids)\n",
        "    embed_transpose = torch.transpose(embed, 2, 3)\n",
        "    embed_transpose = torch.transpose(embed_transpose, 1, 2)\n",
        "\n",
        "\n",
        "    #image\n",
        "    out_conv_layer_1 = self.conv_layer_1(images)\n",
        "\n",
        "    out_conv_block = self.conv_block(out_conv_layer_1)\n",
        "\n",
        "\n",
        "    embed_and_conv = torch.cat((self.avgPool(embed_transpose), self.avgPool(out_conv_block)), 1)\n",
        "\n",
        "    embed_and_conv = self.conv1_1(embed_and_conv)\n",
        "\n",
        "    out_atrous_block = self.atrous_block(embed_and_conv)\n",
        "\n",
        "    out_aspp_0 = self.avgPool(out_atrous_block)\n",
        "    out_aspp_1 = self.avgPool(self.aspp_layer_1(out_atrous_block))\n",
        "    out_aspp_2 = self.avgPool(self.aspp_layer_2(out_atrous_block))\n",
        "    out_aspp_3 = self.avgPool(self.aspp_layer_3(out_atrous_block))\n",
        "\n",
        "    aspp = torch.cat([out_aspp_0, out_aspp_1, out_aspp_2, out_aspp_3], dim=1)\n",
        "\n",
        "    aspp = self.avgPoolFinal(aspp)\n",
        "    aspp = self.conv1_1_layer_2(aspp)\n",
        "    first_layer = self.avgPoolFinal(out_conv_layer_1)\n",
        "    \n",
        "    aspp_and_first_layer = torch.cat([first_layer, aspp], dim=1)\n",
        "\n",
        "    aspp_and_first_layer = self.conv_layer_3(aspp_and_first_layer)\n",
        "    y_hat = self.conv1_1_layer_4(aspp_and_first_layer)\n",
        "\n",
        "    return y_hat\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def eval_net(net, evalloader):\n",
        "  \"\"\"\n",
        "  Function to evaluate the model on unseen data \n",
        "  net: Network CUTIE\n",
        "  evalloader: Dataloader containig data for validation\n",
        "  \"\"\"\n",
        "  \n",
        "  net.eval()\n",
        "  loss_eval = 0\n",
        "\n",
        "  for i, data in enumerate(evalloader, 0):\n",
        "    # get the inputs; data is a list of [inputs, labels]\n",
        "    images, grids, masks = data\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "      # forward \n",
        "      outputs = net(images.to(device), grids.to(device))\n",
        "      loss = criterion(outputs.to(device), masks.squeeze(1))\n",
        "\n",
        "      loss_eval+=loss.item()\n",
        "    net.train()\n",
        "    return loss_eval/len(evalloader)\n",
        "  \n",
        "def apply_transformations(im, grid, mask):\n",
        "\n",
        "  img_trans = transforms.Compose([\n",
        "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "  ])\n",
        "\n",
        "  mask_trans = transforms.Compose([                                   \n",
        "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
        "    # transforms.ToTensor()\n",
        "            \n",
        "  ])\n",
        "  \n",
        "  grid_trans = transforms.Compose([                                   \n",
        "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
        "    # transforms.ToTensor()\n",
        "            \n",
        "  ])\n",
        "\n",
        "  return img_trans(im), grid_trans(grid), mask_trans(mask)\n",
        "\n",
        "def get_boxes(xml_path):\n",
        "    tree = ElementTree.parse(xml_path)\n",
        "    parsed = tree.getroot()\n",
        "\n",
        "    boxes = []\n",
        "\n",
        "    for obj in parsed.findall(\"object\"):\n",
        "\n",
        "        xmin = int(obj.find(\"bndbox/xmin\").text)\n",
        "        ymin = int(obj.find(\"bndbox/ymin\").text)\n",
        "        xmax = int(obj.find(\"bndbox/xmax\").text)\n",
        "        ymax = int(obj.find(\"bndbox/ymax\").text)\n",
        "\n",
        "        boxes.append([xmin, ymin, xmax, ymax])\n",
        "    return boxes\n",
        "\n",
        "\n",
        "def get_mask(boxes, w, h):\n",
        "    \n",
        "    mask = np.zeros((h, w))\n",
        "    positive_label = 1\n",
        "\n",
        "    for xmin, ymin, xmax, ymax in boxes:\n",
        "        mask[ymin: ymax, xmin: xmax] = positive_label\n",
        "\n",
        "    return mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rd8_ZlacqYq5",
        "outputId": "49cc0121-433a-4c8f-8078-e0f4f23ca2fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        }
      ],
      "source": [
        "device = 'cpu'\n",
        "\n",
        "EMBEDDING_SIZE = 32\n",
        "IN_CHANNELS = 3\n",
        "NB_CLASS = 2\n",
        "\n",
        "lr=0.001\n",
        "momentum=0.9\n",
        "batch_size = 4\n",
        "NB_EPOCH = 5\n",
        "\n",
        "val_percent = 0.1\n",
        "\n",
        "# Create the dataset \n",
        "dataset = ReceiptsDataset(root=data_path, transforms = apply_transformations)\n",
        "\n",
        "# Dataloader in order to iterate over the dataset (Batch)\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, sampler=None,\n",
        "           batch_sampler=None, num_workers=0, collate_fn=None,\n",
        "           pin_memory=False, drop_last=False, timeout=0,\n",
        "           worker_init_fn=None,)\n",
        "\n",
        "# Vocab size for the embedding layer\n",
        "VOCAB_SIZE = len(dataset.dictionary.token2id)*255\n",
        "\n",
        "# Split data to (Train, Eval)\n",
        "n_val = int(len(dataset) * val_percent)\n",
        "n_train = len(dataset) - n_val\n",
        "train, val = random_split(dataset, [n_train, n_val])\n",
        "train_loader = DataLoader(train, batch_size=batch_size, shuffle=True, num_workers=8, pin_memory=True)\n",
        "val_loader = DataLoader(val, batch_size=batch_size, shuffle=False, num_workers=8, pin_memory=True, drop_last=True)\n",
        "\n",
        "\n",
        "train_loss_list = []\n",
        "eval_loss_list = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jjP_-JzKJ8RU"
      },
      "outputs": [],
      "source": [
        "def token2id(gensim_dictionary, token):\n",
        "  \"\"\"\n",
        "  Models input are numerical, thus we need to map words to integers\n",
        "  function which maps tokens to integers using gensim dictionary\n",
        "  \"\"\"\n",
        "  return gensim_dictionary.token2id[token]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "An2JEib-J4YF"
      },
      "outputs": [],
      "source": [
        "def get_grid(image, gensim_dictionary):\n",
        "  \n",
        "  # The number of pixels between two tokens (Horizontally)\n",
        "  STRIDE = 4\n",
        "\n",
        "  image = np.array(image)\n",
        "  img_h, img_w, _ = image.shape\n",
        "\n",
        "  grid = np.zeros((img_h, img_w))\n",
        "\n",
        "  custom_config = r'-l eng+it --psm 6'\n",
        "\n",
        "  gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "  thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]\n",
        "\n",
        "  text = pytesseract.image_to_string(thresh, config=custom_config)\n",
        "\n",
        "\n",
        "  line_text_list = text.split(\"\\n\")\n",
        "  line_text_list_tokenized = [word_tokenize(line) for line in line_text_list]\n",
        "\n",
        "  gensim_dictionary.add_documents(line_text_list_tokenized)\n",
        "\n",
        "\n",
        "  nb_sentences = len(line_text_list_tokenized)\n",
        "  nb_token_per_line = [len(line) for line in line_text_list_tokenized]\n",
        "\n",
        " \n",
        "  bboxes_h = int((img_h - 2*nb_sentences) / nb_sentences)\n",
        "  bboxes_per_line_w = ((img_w - 2*np.array(nb_token_per_line))/np.array(nb_token_per_line)).astype(int)\n",
        "  # Building the grid\n",
        "  line_cursor = 0\n",
        "  for line_i in range(nb_sentences):\n",
        "    col_idx = 0\n",
        "\n",
        "    for token_i in line_text_list_tokenized[line_i]:\n",
        "    \n",
        "      grid[line_cursor:line_cursor+bboxes_h, col_idx:col_idx+bboxes_per_line_w[line_i]] = token2id(gensim_dictionary, token_i)\n",
        "      col_idx+=(bboxes_per_line_w[line_i]+STRIDE)\n",
        "    \n",
        "    line_cursor+=(bboxes_h+STRIDE)\n",
        "  \n",
        "  return grid, gensim_dictionary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qbQgwAiwHh21"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "start_time = time.time()\n",
        "# Building and training the model\n",
        "\n",
        "net = CUTIE()\n",
        "net.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=lr, momentum=momentum)\n",
        "\n",
        "for epoch in range(NB_EPOCH):  # loop over the dataset multiple times\n",
        "\n",
        "    # print(\"epoch: \", epoch)\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        images, grids, masks = data\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = net(images.to(device), grids.to(device))\n",
        "        loss = criterion(outputs.to(device), masks.squeeze(1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        loss_eval = eval_net(net, val_loader)\n",
        "\n",
        "        # print(loss.item(), loss_eval)\n",
        "\n",
        "        train_loss_list.append(loss.item())\n",
        "        eval_loss_list.append(loss_eval)\n",
        "    epoch+=1\n",
        "    \n",
        "\n",
        "end_time = time.time()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "orSFnd7uHqG9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "864e6a8c-fa65-45ff-e9bf-bb8d014bdce2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Duration:  594.4108769893646\n"
          ]
        }
      ],
      "source": [
        "print(\"Duration: \", end_time-start_time)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "tskkypxFHyFA",
        "outputId": "c0dd3b34-07cc-4fc9-b7a8-be11016660ed"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Cross entropie loss evolution during train process')"
            ]
          },
          "metadata": {},
          "execution_count": 33
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEICAYAAABYoZ8gAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dd3wVZdbA8d9JJySENFoI0qL0GoqAgmujKFhZEBQsy2JZsb2vru/qWnfRVdfFFTvqgohYVrG3FVAEJFSld0KTGkiAJCSc94+ZuJeYcgMhk9x7vp9PPrkz88zMmXLPzH1m5hlRVYwxxgSPEK8DMMYYU7Us8RtjTJCxxG+MMUHGEr8xxgQZS/zGGBNkLPEbY0yQscRvfiEi94rIyyc47iYROa+yY6pKJ7MMItJERHJEJLSy4yphXjNF5IaTGP9TERlVmTFVJnc9Nvc6jkAW8IlfRK4SkQx3Z9rh7vR9vI7rRJzq5Kqqf1HVE04owaT4tlDVLaoao6qFXsblD1UdoKqvV/Z0RaSfiGw92em463FDZcRkShbQiV9E7gCeBv4C1AeaABOBIaWUD6u66CpfTY/fnFri8PQ7X5320eoUS5VT1YD8A+KAHODKMso8ALwDTAEOAjcAjYAZwD5gHfA7n/LdgQy37M/AU27/KHcae4EsYAFQv5R5NgLeBXYDG4Fbi8UzHfgXkA0sB9LdYZOBY8ARd7n+F2gKKHA9sAWYjXMw/xOwGdjlTivOnUZR+THAdmAHcFex+U/x6e4JfO8u01KgXxnrchNwnvs5EueAu939exqIdIclAR+509wHfAuEuMPuBra5y74aOLeUeUUCT7jL/DPwPFDLHbYSuMinbJi7rru43YPd9ZoFzARal7IMrwGP+AzrB2z1Y1uE+Wzn0vajUrdzKct7PrAKOAD8E5gF3FDKNisex0zgUWCOG29Lt1/R+KOB79z1uR9nnxzgM71mOPtVNvAV8Kzv/HzK1Xanf8xdJznuOniAX3/HugNz3W2ww12mCJ9pKdDSZzs8C3zsxjAfaFHKeipa9rL274p830OBe4H17rwXAqnusFbAl+54q4GhPuMNBFa442wrioEy9v0qz49ezLRKFgz6AwVFX4BSyjwAHAUuwUmYtdydfCJOMu+EkzR+45afC1ztfo4Berqffw98CES7O0tXoE4J8wtxd577gQigObABuNAnnlx3xwkF/grM8xl/E25iKraj/wvni1cLuM7dgZu7Mb4HTC5W/k23fHt3+c7zmf8U93MKzoFsoBv3+W53cinrcpPPdB4C5gH1gGScg8fD7rC/4iTqcPfvLECAM4BMoJFPrKV9wf+O82VNAGLddf9Xd9j9wBs+ZQcBK93PpwOH3GUJx0nY63CTDn4m/nK2RVHCLWs/KnM7F1vWJJwEcoUb8+04+3VFEv8WoC3OQTCcXyf+o8Dv3FhuxEma4rPPP4Gzv/bBSZi/SvwlraMyvmNdcU4qwtx4VwK3+YxTPPHvxTlYhAFvANNKmX/Rspe1f1fk+/4/wI84+6YAHYFEd9qZwLVuTJ2BPUAbd7wdwFnu53j+e9JR4r7vSX70YqZVsmAwAthZTpkHgNk+3alAIRDr0++vwGs+X+YHgaRi07kOJ7l1KGd+PYAtxfr9EXjVJ56vfIa1AY74dG+i5GTT3Kff18BNPt1nuDt7mE/5Vj7DHwde8Zl/UeK/G/eA4VP2c2BUKcv2S2w4Z0gDfYZdCGxyPz8EfID7xfYp0xLnF8p5QHgZ61BwkncLn35nAht9ppMNRLvdbwD3u5/vA6b7jBeCc0bWr4RleI0TTPx+7Edlbudiy3sNxx/8BdhKxRL/Q8WmOZPjE/86n2HR7vgNcKpGC4rWpTt8ChVP/LNLKu9T5jbg3z7dxRP/yz7DBgKrSplO0bKXtX9X5Pu+GhhSwnx+C3xbrN8LwJ/dz1twTgbrFCtT4r7vxV8g1/HvBZL8qMfL9PncCNinqtk+/TbjnP2CU6VyOrBKRBaIyEVu/8k4SXGaiGwXkcdFJLyEeZ0GNBKRrKI/nJ+S9X3K7PT5fBiIOoFl2Fws/rBi88gsNrxRKbFeWSzWPkDDcmIpLYaiefwN5yz7CxHZICL3AKjqOpwE8ACwS0SmiUhJcSXjJKeFPnF95vYvms5K4GIRicap2plaUlyqesxdFylUrvL2I/B/OzfCZ3upk0EySyhXlvLK/xKLqh52P8bw3+U47FO2ovP+1TgicrqIfCQiO0XkIM41uCR/4sNZVzEVmF/x/bsi3/dUnJOY4k4DehT7bozAOVgCXI5zgNosIrNE5Ey3f4n7vhcCOfHPBfJwftaVRX0+bwcSRCTWp18TnLNCVHWtqg7HqcJ4DHhHRGqr6lFVfVBV2wC9gItwztSKy8Q5M63r8xerqgP9XCb1o/92nB3TN/4CnLrwIqnFhm8vJdbJxWKtrarj/YizpBi2A6hqtqreqarNcZLyHSJyrjtsqqr2ccdVnHVc3B6cuuS2PnHFqapvMngTGI5zEX+FezD4VVwiIu662FbCfA7hHGCKNCg2vLRtUTSfUvejCtqBz/byidnfOKHsWMubd4J7AC2SWlrhMuZTvP9zONcs0lS1Ds7Jj5xgjCUpa//2+/uO8x1oUcL0M4FZxb4bMap6I4CqLlDVITh54n2c6zll7vtVLWATv6oewKnvfVZELhGRaBEJF5EBIvJ4KeNk4lTZ/FVEokSkA85Z/hQAERkpIsnumWKWO9oxETlHRNq793AfxKlaOVbCLH4AskXkbhGpJSKhItJORLr5uVg/49Tdl+VN4HYRaSYiMThnU2+paoFPmfvc9dEWp57yrRKmMwXnrPlCN84o93a9xn7E+SbwJxFJFpEknO1QtA4vEpGWbgI7gPNT+5iInCEivxGRSJz676ILhcdx1/1LwN9FpJ47zRQRudCn2DTgApz66qk+/acDg0TkXPcX2Z04Jwffl7AMS4CBIpIgIg1wfo34KnVblLcfVdDHQFsRucz9RXArxyf3JcDZ7nMEcThVh5VCVTfj3MzwgIhEuGeuF5cxys9AohtHWWJxvic5ItIKZztVJn/2b3+208vAwyKS5t4R1UFEEnEu0J4uIle7OSVcRLqJSGt3PY0QkThVPeou5zEofd+v5GX3S8AmfgBVfRK4A+cul904R+pbcI7CpRmOU1e4Hfg3Tr3dV+6w/sByEckB/gEMU9UjOF/Ed3A28kqcuy4mlxBPIc6vgU44d0/swdm5yvuiFPkrTkLNEpG7SikzyZ33bHceucAfipWZhfOT82vgCVX9ooRYM3HOmO/lv+vuf/Bvn3kEJ2Esw7k4tsjtB5CGc3dIDs6vsomq+g3OnTrjcdbJTpyzpdKS2N1u/PPcqoKvcK5lFMW+w512L3y+9Kq6GhgJPOPO52LgYlXNL2Eek3HuZNoEfMGvk0d526Ks/chvqroHuBJn3ezFWX9zfIZ/6ca2DOfGgY8qOo9yjMC5hrIXZxu+hXOwLCnWVTgH/Q3ueimpqg7gLuAqnGsxL1FKYj4J5e7fPsraTk/hnCx8gfPdfgXn7rFsnBOLYe54O3F+nUa6410NbHL3zbE46xBK3/erXNGVexMERKQpzsEgvNgvAGP8IiJv4Vxc/bPXsRRn+7f/AvqM3xhzctwqjBYiEiIi/XF+BZb1i9nUAMH75Joxxh8NcJ4FScS5jfRGVV3sbUjmZPlV1eMe6f+B84DHy8Xv7BCR0Ti3KhVdDf+nqr7sDivEqecF5x72wZUTujHGmBNRbuJ371RZg/O041ac5giGq+oKnzKjcR45v6WE8XOK3WpnjDHGQ/5U9XTHebJvA4CITMO9P/pUBJSUlKRNmzY9FZM2xpiAtXDhwj2qmuxPWX8SfwrHP+22FafpgeIuF5GzcX4d3O7eDgjOE4kZOA8RjVfVMi8MNW3alIyMDD/CMsYYU0RENpdfylFZd/V8CDRV1Q44Ldb5tvV9mqqm49y3+7SI/OpJOBEZI06b+Rm7d++upJCMMcaUxJ/Ev43jH4FuTLFHz1V1r6oWPdTxMk7re0XDipo72IDTOFTn4jNQ1RdVNV1V05OT/fqlYowx5gT5k/gXAGluEwAROE+rzfAtICK+DXcNxnl6FRGJdx/Bx310vzen6NqAMcYY/5Rbx6+qBSJyC07rk6HAJFVdLiIPARmqOgO4VUQG49Tj78Np6hWgNfCCiBzDOciM970byBgTXI4ePcrWrVvJzc31OpQaKyoqisaNGxMeXlIDwP6pdk02pKenq13cNSYwbdy4kdjYWBITE3HaKjMVoars3buX7OxsmjVrdtwwEVnoXk8tlzXZYIypMrm5uZb0T4KIkJiYeNK/mCzxG2OqlCX9k1MZ6y9g2uopKDzG375YTeP4aJokRJMaX4uU+FpEhoV6HZoxxlQrAZP49+Tk8+p3m8gv/O97DUSgfmwUqQm1SE2IJjU+2v1fiyaJ0dSPjSIkxM4+jAkWWVlZTJ06lZtuuqnC4w4cOJCpU6dSt25dv8o/8MADxMTEcNddpb06wzsBk/gbxEWx6uH+/JydS+a+I2TuO0zm/sO/fJ67fi//PrgN32vZEaEhNE+uzaWdU7iia2MSYyJLn4ExpsbLyspi4sSJJSb+goICwsJKT4mffPLJqQytSgVM4gcICREaxtWiYVwtujdL+NXwvIJCtmflsmXf4V8ODAs37eevn67iyS/WMKB9A67q3oTuzRKsHtKYAHTPPfewfv16OnXqxPnnn8+gQYO47777iI+PZ9WqVaxZs4ZLLrmEzMxMcnNzGTduHGPGjAH+25xMTk4OAwYMoE+fPnz//fekpKTwwQcfUKtWrVLnu2TJEsaOHcvhw4dp0aIFkyZNIj4+ngkTJvD8888TFhZGmzZtmDZtGrNmzWLcuHGAU58/e/ZsYmNjS532iQioxF+eyLBQmiXVpllS7eP6r/k5m6nzt/Duoq18sGQ7LevFMKJHEy7r3Ji46BO/V9YYU7oHP1zOiu0HK3WabRrV4c8Xty11+Pjx4/npp59YsmQJADNnzmTRokX89NNPv9weOWnSJBISEjhy5AjdunXj8ssvJzEx8bjprF27ljfffJOXXnqJoUOH8u677zJy5MhS53vNNdfwzDPP0LdvX+6//34efPBBnn76acaPH8/GjRuJjIwkK8t5jfcTTzzBs88+S+/evcnJySEqKupkV8uv2F09wOn1Y3lgcFt+uPc8Hr+iA7Ujw3jwwxV0/8tX3PX2UhZt2U91e97BGFM5unfvftw98RMmTKBjx4707NmTzMxM1q5d+6txmjVrRqdOnQDo2rUrmzZtKnX6Bw4cICsri759+wIwatQoZs+eDUCHDh0YMWIEU6ZM+aWaqXfv3txxxx1MmDCBrKysMqufTlRQnfGXp1ZEKEPTUxmanspP2w4w9YctfLB4G+8s3ErrhnUY0aMJl3ROISbSVpsxJ6usM/OqVLv2f2sAZs6cyVdffcXcuXOJjo6mX79+Jd4zHxn53+uBoaGhHDly5ITm/fHHHzN79mw+/PBDHn30UX788UfuueceBg0axCeffELv3r35/PPPadWq1QlNvzR2xl+Kdilx/OXS9sz/v/N49NJ2APzp/Z/o/uhXPDBjOXtz8sqZgjGmuomNjSU7O7vU4QcOHCA+Pp7o6GhWrVrFvHnzTnqecXFxxMfH8+233wIwefJk+vbty7Fjx8jMzOScc87hscce48CBA+Tk5LB+/Xrat2/P3XffTbdu3Vi1atVJx1CcnbqWIyYyjBE9TuOq7k1YkpnFlHlbmDxvM+8s3MqN/VpwfZ9mRIXbswLG1ASJiYn07t2bdu3aMWDAAAYNGnTc8P79+/P888/TunVrzjjjDHr27Fkp83399dd/ubjbvHlzXn31VQoLCxk5ciQHDhxAVbn11lupW7cu9913H9988w0hISG0bduWAQMGVEoMvqytnhOwblc24z9dxVcrd9EwLoo7LziDyzqn2DMBxpRj5cqVtG7d2uswaryS1qO11XOKtawXy8ujujFtTE+SYyO56+2lXPTMd3y3do/XoRljTLks8Z+Ens0Tef+m3vxjWCcO5h5l5CvzGTXpB1btrNxb1IwxpjJZ4j9JISHCkE4pfH1nX/5vYGsWb9nPwH98y93vLOPng9bmuDHFVbfq5ZqmMtafJf5KEhkWyu/Obs7s/z2Ha3s3473FW+n3t5k89cVqcvIKvA7PmGohKiqKvXv3WvI/QUXt8Z/sQ112cfcU2bL3MI9/voqPlu0gKSaSOy84naHpqYTaBWATxOwNXCevtDdwVeTiriX+U2zxlv08+vFKMjbvp3XDOvz54jb0bJ5Y/ojGGFMBdldPNdK5STxvjz2TZ4Z35uCRowx7cR43TllI5r7DXodmjAlSlvirgIhwccdGfH1nX+44/3Rmrt7NuU/N4vHPVln9vzGmyvmV+EWkv4isFpF1InJPCcNHi8huEVni/t3gM2yUiKx1/0ZVZvA1TVR4KLeem8Y3d/VjUPuGTJy5nnOemMnbGZkcO1a9qtyMMYGr3Dp+EQkF1gDnA1uBBcBwVV3hU2Y0kK6qtxQbNwHIANIBBRYCXVV1f2nzC7Q6/rIs2rKfhz5cwZLMLDo0juP+i9qQ3vTX7xEwxpjyVHYdf3dgnapuUNV8YBowxM9YLgS+VNV9brL/Eujv57gBr0uTeN67sRd//21Hdh3M44rn5/KHNxezLevEWvozxhh/+JP4U4BMn+6tbr/iLheRZSLyjoikVmRcERkjIhkikrF7924/Qw8MISHCpZ0b85+7+nLrb1ryxfKdnPvkTJ76cg2HrP7fGHMKVNbF3Q+BpqraAees/vWKjKyqL6pquqqmJycnV1JINUt0RBh3XHAGX9/Zl/PbNGDC12vp98RM3vxhCwU+L5A3xpiT5U/i3wak+nQ3dvv9QlX3qmpRA/UvA139Hdccr3F8NM8M78y7N/aiSUI0f3zvRwZO+JZvVu2ypx2NMZXCn8S/AEgTkWYiEgEMA2b4FhCRhj6dg4GV7ufPgQtEJF5E4oEL3H6mHF1Pi+edsWfy3Igu5Bcc49rXFjDi5fn8tO2A16EZY2q4cl/EoqoFInILTsIOBSap6nIReQjIUNUZwK0iMhgoAPYBo91x94nIwzgHD4CHVHXfKViOgCQiDGjfkHNb12fq/M384+u1XPTMd1zWOYU7LzyDlLq1vA7RGFMDWZMNNcjB3KNM/GY9k+ZsBOD6Ps24sV8L6kSFlzOmMSbQWVs9AW7r/sM8+cUa/r14Gwm1I7j1Ny25qsdpRITZg9jGBCtrqyfANY6P5u+/7cRHf+hDqwaxPPDhCi74+yz+s+pnr0MzxtQAlvhrsHYpcbxxQw9eHd2N8NAQrnstg79+utJu/zTGlMkSfw0nIpzTqh4f3dqHkT2b8MKsDVz9yg/syckrf2RjTFCyxB8gIsNCeeSS9jxxZUcWbdnPRRO+Y9GWUptEMsYEMUv8AeaKro1576ZehIcJv31hLpPnbbYHv4wxx7HEH4DaNorjo1vOok/LJO57/yfufHspR/ILvQ7LGFNNWOIPUHHR4bwyqhu3nZfGvxdv47Lnvmfz3kNeh2WMqQYs8QewkBDhtvNOZ9LobmzPOsLFz3xnt3waYyzxB4NzzqjHR3/oQ2pCNNe9lsFTX66h0N74ZUzQssQfJFITonn3xl5c0bUxE75ey3WvLSDrcL7XYRljPGCJP4hEhYfytys68Oil7Zi7fi8XPfMdK7Yf9DosY0wVs8QfZESEET1OY/rYMyk8pgx9YS5z1u3xOixjTBWyxB+kOqXW5b2bepFStxajX/2B9xfb+3GMCRaW+INYw7haTB97Jl1Pi+e2t5bw/Kz19rCXMUHAEn+Qi6sVzuvXdeeiDg0Z/+kqHvxwhd3xY0yAK/cNXCbwRYaFMmFYZ+rXieKV7zayKzuXp4Z2Iio81OvQjDGngCV+AzgPe913URsaxkXxyMcr2ZP9Ay9dk05ctL3dy5hAY1U95jg3nNWcCcM7szhzP1c8/z3bs454HZIxppJZ4je/MrhjI16/rjs7D+Ry2cTvWbXT7vU3JpD4lfhFpL+IrBaRdSJyTxnlLhcRFZF0t7upiBwRkSXu3/OVFbg5tXq1SGL62DNRlCufm8vc9Xu9DskYU0nKTfwiEgo8CwwA2gDDRaRNCeVigXHA/GKD1qtqJ/dvbCXEbKpI64Z1eO+m3tSPi2LUpB/4cOl2r0MyxlQCf874uwPrVHWDquYD04AhJZR7GHgMyK3E+IzHUurW4p2xZ9IxNY4/vLmYl7/d4HVIxpiT5E/iTwEyfbq3uv1+ISJdgFRV/biE8ZuJyGIRmSUiZ5U0AxEZIyIZIpKxe/duf2M3VaRudASTr+9B/7YNeOTjlbwwa73XIRljTsJJX9wVkRDgKeDOEgbvAJqoamfgDmCqiNQpXkhVX1TVdFVNT05OPtmQzCkQFR7KsyO6MKhDQ/766Sqm/bDF65CMMSfIn/v4twGpPt2N3X5FYoF2wEwRAWgAzBCRwaqaAeQBqOpCEVkPnA5kVELspoqFhgh/H9qJnNwC7v33j9SpFc7A9g29DssYU0H+nPEvANJEpJmIRADDgBlFA1X1gKomqWpTVW0KzAMGq2qGiCS7F4cRkeZAGmCVxDVYRFgIz43sQucm8Yybtphv11rVnDE1TbmJX1ULgFuAz4GVwHRVXS4iD4nI4HJGPxtYJiJLgHeAsaq672SDNt6Kjghj0qhutEiOYcy/FrJw836vQzLGVIBUt9YY09PTNSPDaoJqgl3ZuVz5/Fz2H8rnrd+fSeuGv7p8Y4ypIiKyUFXT/SlrT+6aE1YvNoop1/egVkQo10z6gc17D3kdkjHGD5b4zUlJTYhmyvU9OFp4jJGvzOfng/YYhzHVnSV+c9LS6sfy2rXd2ZeTz9WvzLeXuBtTzVniN5WiU2pdXromnU17DjP61QUcyivwOiRjTCks8ZtK06tlEs9c1Zkftx1gzOQM8goKvQ7JGFMCS/ymUl3YtgGPXd6BOev2Mu7NJRQUHvM6JGNMMZb4TaW7omtj7r+oDZ8t38kf3/vRXuBuTDVjr140p8R1fZqRdeQoE75eS1hoCP83qDUxkba7GVMd2DfRnDK3n5dG7tFCXpy9gS+W7+TWc9MY3r0JEWH2Q9MYL9k30JwyIsK9A1vz/s29aVkvhj/PWM4Ff5/Fx8t2WPWPMR6yxG9OuU6pdZk2pieTRqcTERbCzVMXccnE75m3wV7naIwXLPGbKiEi/KZVfT4ddzaPX9GBnw/kMuzFeVz/2gLW/JztdXjGBBVrpM14IvdoIZPmbOS5mes5lFfAFV0bc/v5p9MwrpbXoRlTI1WkkTZL/MZT+w/l889v1jF57mZE4Po+zRjbrwV1osK9Ds2YGsVa5zQ1RnztCO67qA1f39mX/u0aMHHmevo+/g2f/bTD69CMCViW+E21kJoQzT+GdeajP/ShSUI0N09dzBfLd3odljEByRK/qVbapcQx5YYetE+J45api5m5epfXIRkTcCzxm2onNiqc16/tTst6Mfx+8kK+X7/H65CMCSiW+E21FBcdzuTru9MkIZobXs9g4WZ7VbMxlcWvxC8i/UVktYisE5F7yih3uYioiKT79PujO95qEbmwMoI2wSExJpI3fteD+nWiGD1pAcu2ZnkdkjEBodzELyKhwLPAAKANMFxE2pRQLhYYB8z36dcGGAa0BfoDE93pGeOXerFRvHFDD+Kiw7n6lR9YueOg1yEZU+P5c8bfHVinqhtUNR+YBgwpodzDwGOA70tXhwDTVDVPVTcC69zpGeO3RnVr8ebvelIrPJSRL89n3a4cr0MypkbzJ/GnAJk+3Vvdfr8QkS5Aqqp+XNFx3fHHiEiGiGTs3r3br8BNcElNiGbq73ogIox4eR6b9x7yOiRjaqyTvrgrIiHAU8CdJzoNVX1RVdNVNT05OflkQzIBqnlyDG/c0IP8gmNc9dJ8tmUd8TokY2okfxL/NiDVp7ux269ILNAOmCkim4CewAz3Am954xpTIWc0iGXy9T04mHuUq16ax88Hc8sfyRhzHH8S/wIgTUSaiUgEzsXaGUUDVfWAqiapalNVbQrMAwaraoZbbpiIRIpIMyAN+KHSl8IElXYpcbx+XXf2ZOdx1Uvz2JOT53VIxtQo5SZ+VS0AbgE+B1YC01V1uYg8JCKDyxl3OTAdWAF8BtysqoUnH7YJdl2axDNpdDe2ZR1h5MvzyTqc73VIxtQY1jqnqdG+Xbub61/LoFXDWN64oQex1qqnCVLWOqcJGmelJTNxRBdWbD/I2CkLySuwH5TGlMcSv6nxzmtTn8cu78CcdXu5c/pSjh2rXr9ijaluwrwOwJjKcHnXxuzOyWP8p6tIjo3k/ovaICJeh2VMtWSJ3wSM35/dnJ8P5vLqnE3UrxPF2L4tvA7JmGrJEr8JGCLCfYPasDvbPfOPieTyro29DsuYascSvwkoISHCk0M7su9QPne/u4zEmAj6nVHP67CMqVbs4q4JOJFhobxwdVdOrx/LTW8sYmmmNedsjC9L/CYgxUaF89p13UiMieDa1xawcY816mZMEUv8JmDVi43i9WudVsCvmTSfXdnWro8xYInfBLjmyTG8Orobe7LzGT1pAdm5R70OyRjPWeI3Aa9jal2eG9mFNT9n8/vJ9nSvMZb4TVDod0Y9Hr+iA9+vt6d7jbHbOU3QuKxLY3Zl29O9xljiN0HF9+neiLAQru55Go3jo70Oy5gqZYnfBJWip3v3H8rnhVkbeGHWBpon1eastCT6pCVzZotEYiLta2ECm7XHb4KSqrJ+dw6z1+zh27W7mbdhH0eOFhIWInRpEs9ZaUmcdXoy7VPiCA2x6iBT/VWkPX5L/MYAeQWFLNy8n2/X7uG7tXv4cdsBAOJqhdOnZZL7iyDJqoVMtWWJ35iTtDcnjznr9/Ltmt18u3YPO92Xuo/s2YSHh7Szi8Km2qlI4rfKTGNKkBgTyeCOjRjcsdEv1UKvztnElHlbaJEcw7W9m3kdojEnzBK/MeUQEVrWi+XhIe3YlZ3HIx+vJK1eLH3SkrwOzZgT4tcDXCLSX0RWi8g6EbmnhOFjReRHEVkiIt+JSBu3f1MROeL2X29HGf8AABP9SURBVCIiz1f2AhhTVUJChL//thMtk2O4eeoiNlnDb6aGKjfxi0go8CwwAGgDDC9K7D6mqmp7Ve0EPA485TNsvap2cv/GVlbgxnghJjKMl65JRwRu+FeGtf1jaiR/zvi7A+tUdYOq5gPTgCG+BVT1oE9nbaB6XTE2phI1SYxm4ogubNxziNumLaHQmn8wNYw/iT8FyPTp3ur2O46I3Cwi63HO+G/1GdRMRBaLyCwROaukGYjIGBHJEJGM3bt3VyB8Y7zRq0USD1zchq9X7eLJL1Z7HY4xFVJpjbSp6rOq2gK4G/iT23sH0ERVOwN3AFNFpE4J476oqumqmp6cnFxZIRlzSo3seRrDuzdh4sz1fLBkm9fhGOM3fxL/NiDVp7ux268004BLAFQ1T1X3up8XAuuB008sVGOqFxHhwcFt6d40gf99ZxnLttorHk3N4E/iXwCkiUgzEYkAhgEzfAuISJpP5yBgrds/2b04jIg0B9KADZURuDHVQURYCM+N7EJSTCRj/rXQ3vJlaoRyE7+qFgC3AJ8DK4HpqrpcRB4SkcFusVtEZLmILMGp0hnl9j8bWOb2fwcYq6r7Kn0pjPFQYkwkL12TzoEjRxlrL3oxNYA12WBMJfn0xx3c+MYirujamL9d0cGadTBVqiJNNtgbuIypJAPaN2TcuWm8s3Ark+Zs8jocY0plid+YSjTu3DQubFufRz9ewew1dmuyqZ4s8RtTiUJChKeGduL0+rHcMnURG61ZB1MNWeI3ppLVdpt1CAsN4YbXF7A7O8/rkIw5jiV+Y06B1ASnWYet+49w4dOz+eynHV6HZMwvLPEbc4r0bJ7IR3/oQ0rdWoydsog73lrCgSPWqJvxniV+Y06htPqxvHdTL8adm8YHS7fT/+nZfLd2j9dhmSBnid+YUyw8NITbzz+d927sRa2IUEa+Mp8HZiznSL496GW8YYnfmCrSMbUun9x6Ftf2bspr329i0IRvWZJp7fuYqmeJ35gqFBUeyp8vbsvUG3qQe7SQy5/7nqe+WM3RwmNeh2aCiCV+YzzQq2USn91+Npd0SmHCf9Zx6cQ5rPk52+uwTJCwxG+MR+pEhfPk0I48P7IrO7JyueiZ73j52w0cszd6mVPMEr8xHuvfrgGf3342fU9P5pGPV3L1pPkczi/wOiwTwCzxG1MNJMVE8uLVXRl/WXvmrt/LjVMWkV9g9f7m1LDEb0w1ISIM696Ev1zanllrdnPX20ut2secEmFeB2CMOd6w7k3Ydzifxz9bTXx0OA8Mbmtt+5tKZYnfmGroxr4t2H8on5e+3UhC7UjGnZdW/kjG+MkSvzHVkIhw78DW7Dt0lL9/tYaE2uFcfWZTr8MyAcISvzHVlIjw2OXtOXAkn/tnLCcuOoLBHRt5HZYJAH5d3BWR/iKyWkTWicg9JQwfKyI/isgSEflORNr4DPujO95qEbmwMoM3JtCFhYbwz6u60O20BO6cvoRZ9lYvUwnKTfwiEgo8CwwA2gDDfRO7a6qqtlfVTsDjwFPuuG2AYUBboD8w0Z2eMcZPUeGhvDw6nZb1Yhk7eSGLtuz3OiRTw/lzxt8dWKeqG1Q1H5gGDPEtoKoHfTprA0X3oA0BpqlqnqpuBNa50zPGVECdqHBev64b9epEct1rC6x5B3NS/En8KUCmT/dWt99xRORmEVmPc8Z/awXHHSMiGSKSsXu3/ZQ1piT1YqOYfF0PwkNDuOaVH9i6/7DXIZkaqtIe4FLVZ1W1BXA38KcKjvuiqqaranpycnJlhWRMwGmSGM2/ruvOofwCrnnlB/bm2Pt8TcX5k/i3Aak+3Y3dfqWZBlxyguMaY8rRumEdJo3uxrasI4x+dQHZufY6R1Mx/iT+BUCaiDQTkQici7UzfAuIiO/TJYOAte7nGcAwEYkUkWZAGvDDyYdtTHDr1jSB50Z2YcWOg4z510Jyj9rbvIz/yr2PX1ULROQW4HMgFJikqstF5CEgQ1VnALeIyHnAUWA/MModd7mITAdWAAXAzapqe6gxleA3rerzxJUduP2tpZz75Cz6tEyid1oSvVokkhQT6XV4phoT1erVCFR6erpmZGR4HYYxNcZnP+3kvUVbmbthL9m5TnPOrRrE0rtlEr1bJtK9WSIxkfasZqATkYWqmu5XWUv8xgSGwmPKj9sOMGfdHuas20PG5v3kFxwjLETolFrXPRAk0Sm1LhFh1jBvoLHEb4wh92ghGZv2M2f9Hr5ft4dl2w6gCtERofRsnsi9A1vRsl6s12GaSmKJ3xjzKwcOH2Xuhr3MWbeHj3/cQeEx5eVR6XRrmuB1aKYSVCTx2+89Y4JEXHQ4/ds14OFL2vHBzb1JrB3BiJfn89lPO7wOzVQxS/zGBKHUhGjeubEX7RrV4cY3FvH695u8DslUIUv8xgSphNoRvHFDT85tVZ8/z1jOY5+torpV/ZpTwxK/MUGsVkQoz4/swogeTXhu5nrunL7UXvIeBOzmXmOCXFhoCI9c0o6GcVE88cUadufk8dzIrnbvfwCzM35jDCLCLb9J429XdOD79Xv57Qtz2XUw1+uwzCliid8Y84sr01N5ZVQ6G/cc4rLnvmf97hyvQzKngCV+Y8xx+p1Rj2ljepJ7tJDLn/uehZvtjV+BxhK/MeZXOjSuy3s39qZurXCuemkeXyzf6XVIphJZ4jfGlKhJYjTv3tiLVg3rMHbKQibP22y3ewYIS/zGmFIlxkTy5u96cM4Z9bjv/Z+47a0lHLQXv9R4lviNMWWKjgjjxWvSufP80/lo2Q4G/uNbq/ev4SzxG2PKFRoi/OHcNKb//kwAhr4wlwlfr6XwmFX91ESW+I0xfut6WjyfjDuLQe0b8tSXaxj+4jy2ZR3xOixTQZb4jTEVUicqnH8M68RTQzuyfPsBBjw9m09+tBY+axJL/MaYChMRLuvSmE/GnUWz5BhuemMR97y7jMP5BV6HZvxgid8Yc8JOS6zNO2PP5KZ+LXgrI5OLJnzHT9sOeB2WKYdfiV9E+ovIahFZJyL3lDD8DhFZISLLRORrETnNZ1ihiCxx/2ZUZvDGGO+Fh4bwv/1b8cYNPTicX8ilE+fw0uwNHLMLv9VWuYlfREKBZ4EBQBtguIi0KVZsMZCuqh2Ad4DHfYYdUdVO7t/gSorbGFPN9GqRxKfjzuKcM+rx6CcrGfXqD3bht5ry54y/O7BOVTeoaj4wDRjiW0BVv1HVw27nPKBx5YZpjKkJ4mtH8MLVXXn00nYs2LSP3uP/wyXPzuG5mevZYA2+VRv+NLidAmT6dG8FepRR/nrgU5/uKBHJAAqA8ar6fvERRGQMMAagSZMmfoRkjKmuRIQRPU7j7LRkPliyjc+X/8xjn63isc9WkVYvhv7tGnBh2wa0bVQHEfE63KAk5bW9ISJXAP1V9Qa3+2qgh6reUkLZkcAtQF9VzXP7pajqNhFpDvwHOFdV15c2v/T0dM3IyDjhBTLGVD/bso7wxfKdfL58Jz9s3McxhZS6tbigbX0ubNuAbk0TCA2xg8DJEJGFqpruT1l/zvi3Aak+3Y3dfsVneh7wf/gkfQBV3eb+3yAiM4HOQKmJ3xgTeFLq1uLa3s24tncz9ubk8fXKXXy+fCdvzN/Cq3M2kVA7gvNa1+PCtg3o3iyB2Khwr0MOaP6c8YcBa4BzcRL+AuAqVV3uU6YzzkXd/qq61qd/PHBYVfNEJAmYCwxR1RWlzc/O+I0JHjl5BcxavZvPl+/km1W7yM4rIETg9PqxdD0t/pe/JgnRVi1Ujoqc8Zeb+N0JDgSeBkKBSar6qIg8BGSo6gwR+QpoDxQ9vrdFVQeLSC/gBeAYzoXkp1X1lbLmZYnfmOCUV1DIDxv3kbFpP4u27Gfxlixy8pwHwpJiIujSJJ4u7oGgfUocUeGhHkdcvVR64q9KlviNMQCFx5S1u7JZuHk/CzfvZ9Hm/Wza69w8GB4qtG0UR5cm8ZzTKpmz0pI9jtZ7lviNMQFpT04ei7dk/XIgWLo1i7yCY7w6uhvntKrndXiessRvjAkKuUcLGfLPOew/nM8Xt59N3egIr0PyTEUSv7XVY4ypsaLCQ3lyaEf2Hcrnvg+Wlz+CASzxG2NquHYpcdx6bhofLt3OR8u2ex1OjWCJ3xhT493UrwUdG8dx3/s/sSs71+twqj1L/MaYGi8sNIQnh3bicH4hf3z3R6rbtcvqxhK/MSYgtKwXw/9ceAZfr9rF2xlbvQ6nWrPEb4wJGNf1bkaPZgk89NEKtu4/XP4IQcoSvzEmYISECE9c2RFV5X/eXmYvgymFJX5jTEBJTYjmvovaMHfDXl6fu8nrcKolS/zGmIDz226pnHNGMuM/XcV6ewHMr1jiN8YEHBFh/OUdiAoP5Y7pSykoPOZ1SNWKJX5jTECqXyeKhy9px9LMLF6YvcHrcKoVS/zGmIA1uGMjBnVoyNNfrWHF9oNeh1NtWOI3xgS0R4a0I65WBHdMX0JeQaHX4VQLlviNMQEtvnYEj13enlU7s3n6q7XljxAELPEbYwLeua3rMzS9MS/MWs/Czfu9DsdzlviNMUHhvova0DCuFne9vZTD+QVeh+OpMK8DMMaYqhAbFc7fruzAVS/N5/Ln5tKyXgzJMZEkxUaQFBNJckwkybGRJMVEkhgTQXho4J4X+5X4RaQ/8A+cl62/rKrjiw2/A7gBKAB2A9ep6mZ32CjgT27RR1T19UqK3RhjKqRXiyQeHNyWGUu3szQziz05eRzOL/mCb93ocJJiIkmKiaB+nSh+m55Kr5ZJVRzxqVHuqxdFJBRYA5wPbAUWAMNVdYVPmXOA+ap6WERuBPqp6m9FJAHIANIBBRYCXVW11Eo2e/WiMaYqHc4vYE92Prtz8thT9Jed/9/POXls3HOIPTn5XNyxEX8a1Jr6daK8DvtXKvLqRX/O+LsD61R1gzvxacAQ4JfEr6rf+JSfB4x0P18IfKmq+9xxvwT6A2/6E5wxxpxq0RFhNEkMo0lidKllco8W8vys9UycuZ5vVu3itvPSGN2rKWE1tDrIn6hTgEyf7q1uv9JcD3x6guMaY0y1ExUeym3nnc6Xt59NetN4Hvl4JRc98x0LNu3zOrQTUqmHKxEZiVOt87cKjjdGRDJEJGP37t2VGZIxxlSa0xJr8+robjw/sisHjxzlyufnctfbS9mTk+d1aBXiT+LfBqT6dDd2+x1HRM4D/g8YrKp5FRlXVV9U1XRVTU9OTvY3dmOMqXIiQv92Dfjqzr7c2K8FHyzZxm+emMnkeZsprCHt//uT+BcAaSLSTEQigGHADN8CItIZeAEn6e/yGfQ5cIGIxItIPHCB288YY2q06Igw7u7fik/HnU27FOdF75dOnMPSzCyvQytXuYlfVQuAW3AS9kpguqouF5GHRGSwW+xvQAzwtogsEZEZ7rj7gIdxDh4LgIeKLvQaY0wgaFkvhjdu6MGE4Z3ZeSCXSybO4d5//0jW4XyvQytVubdzVjW7ndMYU1Nl5x7l6a/W8tr3mwgPFRrG1SKxdsQvD4UVPRfgdDufE2MiqRMVhoic1LwrcjunJX5jjKlkK3cc5O2Mrc6zAdl57D2Ux56cfPYfzqeklBsRGkJiTATpTRN4ZnjnE5pnZd/Hb4wxpgJaN6zD/Re3+VX/gsJj7Ducz57sfPdgkMfeHOfhsb05+dSLjayS+CzxG2NMFQkLDaFebBT1Yr198rdmPnZmjDHmhFniN8aYIGOJ3xhjgowlfmOMCTKW+I0xJshY4jfGmCBjid8YY4KMJX5jjAky1a7JBhHZDWw+iUkkAXsqKZyaxpY9eAXz8gfzssN/l/80VfWrXftql/hPlohk+NteRaCxZQ/OZYfgXv5gXnY4seW3qh5jjAkylviNMSbIBGLif9HrADxkyx68gnn5g3nZ4QSWP+Dq+I0xxpQtEM/4jTHGlMESvzHGBJmASfwi0l9EVovIOhG5x+t4qpqIbBKRH92X3Qf0uytFZJKI7BKRn3z6JYjIlyKy1v0f72WMp1Ipy/+AiGxzt/8SERnoZYynioikisg3IrJCRJaLyDi3f8Bv/zKWvcLbPiDq+EUkFFgDnA9sBRYAw1V1haeBVSER2QSkq2rAP8giImcDOcC/VLWd2+9xYJ+qjncP/PGqereXcZ4qpSz/A0COqj7hZWynmog0BBqq6iIRiQUWApcAownw7V/Gsg+lgts+UM74uwPrVHWDquYD04AhHsdkThFVnQ3sK9Z7CPC6+/l1nC9EQCpl+YOCqu5Q1UXu52xgJZBCEGz/Mpa9wgIl8acAmT7dWznBFVKDKfCFiCwUkTFeB+OB+qq6w/28E6jvZTAeuUVElrlVQQFX1VGciDQFOgPzCbLtX2zZoYLbPlASv4E+qtoFGADc7FYHBCV16i9rfh1mxTwHtAA6ATuAJ70N59QSkRjgXeA2VT3oOyzQt38Jy17hbR8oiX8bkOrT3djtFzRUdZv7fxfwb5zqr2Dys1sHWlQXusvjeKqUqv6sqoWqegx4iQDe/iISjpP43lDV99zeQbH9S1r2E9n2gZL4FwBpItJMRCKAYcAMj2OqMiJS273Yg4jUBi4Afip7rIAzAxjlfh4FfOBhLFWuKOm5LiVAt7+ICPAKsFJVn/IZFPDbv7RlP5FtHxB39QC4tzA9DYQCk1T1UY9DqjIi0hznLB8gDJgayMsvIm8C/XCao/0Z+DPwPjAdaILTrPdQVQ3IC6ClLH8/nJ/6CmwCfu9T5x0wRKQP8C3wI3DM7X0vTl13QG//MpZ9OBXc9gGT+I0xxvgnUKp6jDHG+MkSvzHGBBlL/MYYE2Qs8RtjTJCxxG+MMUHGEr8xxgQZS/zGGBNk/h8whSnNrVN4MwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.plot(train_loss_list, label=\"train loss\")\n",
        "#plt.plot(eval_loss_list, label=\"val loss\")\n",
        "plt.legend()\n",
        "plt.title(\"Cross entropie loss evolution during train process\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ORFCuUIFH0un",
        "outputId": "eb22ca9d-28bb-4aef-82b7-5d0ce06872a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:30: RuntimeWarning: divide by zero encountered in true_divide\n"
          ]
        }
      ],
      "source": [
        "receipt_sample = \"/content/sample_data/invoices/receipts/20210111_TNH_IUGU_NFS_412057-1.jpg\"\n",
        "net.eval()\n",
        "\n",
        "img = Image.open(receipt_sample).convert(\"RGB\")\n",
        "w,h = img.size\n",
        "grid, dictionary = get_grid(img, dataset.dictionary)\n",
        "\n",
        "grid = Image.fromarray(np.uint8(cm.gist_earth(grid)*255)).convert(\"L\")\n",
        "\n",
        "\n",
        "img_trans = transforms.Compose([\n",
        "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "  ])\n",
        "\n",
        "  \n",
        "grid_trans = transforms.Compose([                                   \n",
        "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
        "    # transforms.ToTensor()\n",
        "])\n",
        "img = img_trans(img)\n",
        "\n",
        "grid = grid_trans(grid)\n",
        "\n",
        "img = torch.as_tensor(np.array(img), dtype=torch.float32)\n",
        "grid = torch.as_tensor(np.array(grid), dtype=torch.long)\n",
        "\n",
        "img = img.unsqueeze(0)\n",
        "grid = grid.unsqueeze(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "id": "LJz6wUZcH3FT",
        "outputId": "8b7e116b-5dac-40a9-8016-48d56be99329"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 300, 300)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAASwAAAEsCAIAAAD2HxkiAAABHUlEQVR4nO3BAQ0AAADCoPdPbQ43oAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAIAnAyAYAAFGw74oAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=300x300 at 0x7F0666276190>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "  mask_pred = net(img, grid)\n",
        "  mask_np = np.argmax(mask_pred.numpy(),1)\n",
        "  print(mask_np.shape)\n",
        "  mask_np = np.transpose(mask_np, (1,0,2))\n",
        "  mask_np = np.transpose(mask_np, (0,2,1))\n",
        "\n",
        "  cv2_imshow(mask_np)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt install tesseract-ocr\n",
        "!pip install pytesseract"
      ],
      "metadata": {
        "id": "evFjD9vFJ8Ow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.insert(1, './drive/MyDrive/UBIAI_layoutlm')\n",
        "from layoutlm_preprocess import *\n",
        "image_path='./content/invoice_test.jpg'\n",
        "image, words, boxes, actual_boxes = preprocess(image_path)"
      ],
      "metadata": {
        "id": "XJKY9uDTJ-3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_path='./drive/MyDrive/trained_layoutlm/layoutlm_UBIAI.pt'\n",
        "model=model_load(model_path,num_labels)\n",
        "word_level_predictions, final_boxes=convert_to_features(image, words, boxes, actual_boxes, model)"
      ],
      "metadata": {
        "id": "FRA0uOcfKCIv"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "CUTIE_invoice.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}